---
title: "Assessing Market Risk of S&P 500 Companies Using Support Vector Machines"
author: "Caleb Boateng, Lillian Caldwell"
date: "May 20, 2024"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
    citation_package: natbib
bibliography: "https://api.citedrive.com/bib/b6112856-78fb-48b8-8d4b-e5264d4654ee/references.bib?x=eyJpZCI6ICJiNjExMjg1Ni03OGZiLTQ4YjgtOGQ0Yi1lNTI2NGQ0NjU0ZWUiLCAidXNlciI6ICI3NDI5IiwgInNpZ25hdHVyZSI6ICJkYTlhNjJhZWNjZTkxOWFjMDIwMDNjNWQ2N2YxZmUwYzM5YWNmZmIyMTA5NWI2YzBjZTE1NTE3ZjhiZWRkNGYzIn0="
---

# Introduction
Market risk, or systematic risk, is the potential for losses due to factors like interest rate changes, exchange rates, recessions, political instability, and natural disasters.[@Hayes] It’s crucial for informed financial decisions and strategies to mitigate risk. The primary objective of this project is to develop a predictive model to assess the market risk of publicly traded companies in the S&P 500 using beta and other financial indicators. Specifically, this project will categorize companies into high risk and low risk based on their beta values using a Support Vector Machine (SVM) in the classification process. 
The dataset used in this analysis consists of financial indicators for companies in the S&P 500. Key variables include beta, price-to-earnings ratio, earnings per share, dividend yield, and market capitalization. Beta is a statistical measure of a stock’s volatility in relation to the overall market.[@Liberto] A beta greater than 1.0 indicates that a stock is more volatile than the market, and a beta less than 1.0 indicates that a stock is less volatile than the broader market.[@Liberto] Investors use beta values to assess the riskiness of an investment. A higher beta means higher risk, but also higher returns. The price-to-earnings ratio is a proportion of a company’s share price to its earnings per share. The P/E ratio helps investors compare the price of a company’s stock to the earnings a company generates. The definition of earnings per share is intuitive: a company’s net income divided by its number of outstanding common shares. In other words, how much a company makes per stock. Dividend yield is a ratio indicating how much income earned in dividend payouts per year for every dollar invested in a stock. This ratio informs investors of what the annual return on investment would be at the price paid for the security.[@Tretina] Lastly, market capitalization shows how much a company is worth through the total value of all its shares outstanding. All these variables provide insight to investors on the risk of investments in different publicly traded companies. The data was produced by combining ‘S&P 500 Stocks’ from kaggle with the financial metrics taken from the Yahoo finance API for python.[@Larxel]

Support Vector Machines (SVM) are a type of machine learning algorithm used for classification tasks. Classification involves predicting which category, or class, a data point belongs to. In this case, the SVM algorithm will aim to classify companies as either high risk or low risk based on their financial data. But how exactly do Support Vector Machines work? Think of a hyperplane as a line that separates different groups of data points in space. In a simple two-dimensional plot, this would be just a straight line. In this context, the hyperplane separates companies into high-risk and low-risk categories based on their financial indicators. The SVM algorithm finds the hyperplane that leaves the largest possible space, or margin, between the high-risk and low-risk companies, contributing to better predictions. The support vectors are the data points that are closest to the hyperplane. These points are crucial because they determine the position and orientation of the hyperplane. For this project, these would be the financial indicators of companies that are closest to the decision boundary between high risk and low risk. This project will use a linear kernel, which is like drawing a straight line in a plot to separate the two classes. This approach is simple and effective for the data. 

# Methodology
We collected and cleaned data, then scaled it for analysis. Additionally, SVM works best with classification tasks. Thus, in order to get the best results, beta was converted into a categorical variable named RiskCategory. Any beta that exceeds 1 implies higher risk, and any beta lower than 1 implies lower risk.

## Data Preprocessing
```{r}
library(readr)
data <- read_csv("Desktop/Stat Learning/tech_company_data3.csv")

```

## Cleaning and Scaling Data

```{r data-preprocessing, echo=TRUE}
library(dplyr)
library(caTools)
library(tidyr)

cleaned_data =drop_na(data)

no_tick <- cleaned_data |> dplyr::select(-ticker)
no_tick$RiskCategory <- ifelse(no_tick$beta < 1, "Low Risk", "High Risk")
no_tick$beta <- NULL


data_scaled <- scale(no_tick[, -ncol(no_tick)]) 
data_scaled <- as.data.frame(data_scaled)
data_scaled$RiskCategory <- no_tick$RiskCategory
data_scaled$RiskCategory <- factor(data_scaled$RiskCategory, levels = c("Low Risk", "High Risk"))
data_scaled <- na.omit(data_scaled)

```

## Exploratory Data Analysis
```{r}
summary(data_scaled)
```

# Model Implementations
Start by splitting the data into train and test data sets. This project used a split ratio of 0.8, meaning 80% of the original data set was used for training data, while the remaining 20% will be test data, used to test the accuracy of a trained algorithm. The e1071 package contains tools for SVM classification and regression. The ‘svm’ function of the e1071 library is used to train a support vector machine. The ‘svm’ function can carry out general regression and classification, but in this project it will be used for classification purposes. The ‘svm’ function takes on the following arguments: formula, data, x, y, scale, type, kernel, and many more. For this project, only 4 arguments will be used to train the ‘svm’ function: formula, data, type, and kernel. ‘Formula’ simply refers to the model description, or the factors that will be included in the model. ‘Data’ refers to the data set containing the factors of the model. ‘Type’ refers to the type of classification machine used in the model. SVM can be used as a classification machine, a regression machine, or for novelty detection. For this project, ‘type’ will be set to equal ‘C-classification.’ Lastly, ‘kernel’ determines the kernel used in training and predicting. Here, kernel  = ‘linear.’ 

In evaluating the effectiveness of the SVM model, it can be helpful to compare its accuracy to another statistical learning algorithm. This project uses an LDA model for this purpose. LDA, or Linear Discriminant Analysis, is a statistical learning approach used to solve multi-class classification problems. LDA looks for a linear combination of the features (financial indicators) that can best separate the high-risk companies from the low-risk companies. Start by attaching the ‘MASS’ package, which includes the ‘lda’ function. The ‘lda’ function takes on similar arguments to the ‘svm’ function, and for this project only ‘formula’ and ‘data’ were used to train the model. 

To evaluate both models, the LDA and SVM models were used to make predictions on the test data. Starting with the SVM model, we used the ‘predict’ function to make predictions on the test data. Next, we installed the ‘caret’ packages to access the ‘confusionMatrix’ function. This function calculates a cross-tabulation of observed and predicted classes with associated statistics. We then created a confusion matrix of the SVM model predictions and the actual risk category values of the test data. This process was repeated to create LDA model predictions and a LDA confusion matrix. 

## Splitting the Data

```{r}
set.seed(123)
split <- sample.split(data_scaled$RiskCategory, SplitRatio = 0.8)
train_data <- data_scaled[split, ]
test_data <- data_scaled[!split, ]

train_data$RiskCategory <- as.factor(train_data$RiskCategory)
test_data$RiskCategory <- as.factor(test_data$RiskCategory)

```

## Training the SVM Model
```{r}
library(e1071)


svm_model <- svm(RiskCategory ~ ., data = train_data, type = 'C-classification', kernel = 'linear')

```

## Training LDA Model

```{r}
library(MASS)

lda_model <- lda(RiskCategory ~ ., data = train_data)

```

## Model Evaluation
```{r}

svm_predictions <- predict(svm_model, test_data)


library(caret)
svm_confusion_matrix <- confusionMatrix(svm_predictions, test_data$RiskCategory)

lda_predictions <- predict(lda_model, test_data)
lda_predicted_classes <- lda_predictions$class

lda_confusion_matrix <- confusionMatrix(lda_predicted_classes, test_data$RiskCategory)

```

# Results

```{r}
svm_confusion_matrix

```

```{r}
lda_confusion_matrix
```

The SVM model performed well, but the LDA model showed slightly better accuracy.

# Comparison of SVM and LDA

To better capture and visualize the differences and similarities between high-risk and low-risk classes after creating the two models, we created 2D boundary plots using principal component analysis, or PCA. PCA is a dimensionality reduction method that is used to reduce the dimensionality of large data sets. Performing PCA allows for the ability of visualizing relationships of high dimensional data in a 2D or 3D space. To perform principal component analysis, we used the ‘prcomp’ function, which performs a PCA on the given data matrix and returns the reduced results. The PCA
results were then put in a data frame to access later. 

Next, new SVM and LDA models were trained on the PCA-reduced data. After training the SVM and LDA models, we created a grid of points to plot decision boundaries. To create the grid, we used the ‘seq’ function to find the range of the first principal component and the second principal component. We then used the ‘expand.grid’ function to make a grid of both these ranges. Next, we predicted the classes for the grid points and plotted the decision boundaries using ‘ggplot’. The ‘ggplot2’ and ‘gridExtra’ libraries are required to plot the decision boundaries. From this process, two plots are created, a SVM decision boundary plot and a LDA decision boundary plot. To better compare both plots, we decided to combine them. 


```{r}
library(e1071)
library(MASS)
library(ggplot2)
library(gridExtra)
```

## Perform PCA
```{r}
pca <- prcomp(data_scaled[, -ncol(data_scaled)], scale. = TRUE)
data_pca <- as.data.frame(pca$x[, 1:2])
data_pca$RiskCategory <- data_scaled$RiskCategory
```

## Train SVM on PCA-reduced data
```{r}
svm_model_pca <- svm(RiskCategory ~ ., data = data_pca, type = 'C-classification', kernel = 'linear')
```

## Train LDA on PCA-reduced data
```{r}
lda_model_pca <- lda(RiskCategory ~ ., data = data_pca)
```

## Create a grid of points to plot decision boundaries
```{r}
xrange <- seq(min(data_pca$PC1), max(data_pca$PC1), length.out = 200)
yrange <- seq(min(data_pca$PC2), max(data_pca$PC2), length.out = 200)
grid <- expand.grid(PC1 = xrange, PC2 = yrange)
```

## Predict classes for grid points
```{r}
svm_grid_pred <- predict(svm_model_pca, grid)
lda_grid_pred <- predict(lda_model_pca, grid)$class
```

## Plot decision boundaries
```{r}
plot_svm <- ggplot() +
  geom_tile(data = grid, aes(x = PC1, y = PC2, fill = as.factor(svm_grid_pred)), alpha = 0.3) +
  geom_point(data = data_pca, aes(x = PC1, y = PC2, color = RiskCategory)) +
  labs(title = "SVM Decision Boundary", x = "Principal Component 1", y = "Principal Component 2") +
  scale_fill_manual(values = c("Low Risk" = "lightblue", "High Risk" = "pink"), name = "Predicted") +
  scale_color_manual(values = c("Low Risk" = "blue", "High Risk" = "red"), name = "Actual")

plot_lda <- ggplot() +
  geom_tile(data = grid, aes(x = PC1, y = PC2, fill = as.factor(lda_grid_pred)), alpha = 0.3) +
  geom_point(data = data_pca, aes(x = PC1, y = PC2, color = RiskCategory)) +
  labs(title = "LDA Decision Boundary", x = "Principal Component 1", y = "Principal Component 2") +
  scale_fill_manual(values = c("Low Risk" = "lightgreen", "High Risk" = "orange"), name = "Predicted") +
  scale_color_manual(values = c("Low Risk" = "green", "High Risk" = "orange"), name = "Actual")
```

## Combine plots

```{r}
grid.arrange(plot_svm, plot_lda, ncol = 2)
```

This type of plot helps visualize how the SVM and LDA models separate the two classes based on their decision boundaries. It is especially effective when combined with the PCA to reduce dimensionality of the data to dimensions. The SVM model predicts most of the companies at high risk, as indicated by the red shading across almost the entire plot. This results in many low-risk companies being misclassified as high risk. The model does not effectively capture the separation between high-risk and low-risk companies in the PCA-reduced space. The LDA model provides a clearer and more balanced separation between high-risk and low-risk companies. The decision boundary effectively differentiates the two classes, with fewer misclassifications. The green and orange regions show a more logical separation based on the PCA components. 


# Discussion, Future Work, and Conclusion

One of the primary strengths of SVM is its effectiveness with high-dimensional data. The algorithm provides a clear margin of separation between classes. In this project, the SVM model demonstrated a strong ability to classify companies into high-risk and low-risk categories based on different financial indicators. Similarly, LDA was also a strong algorithm to use for market risk predictions. LDA is a robust method when the class distributions are Gaussian with identical covariances. For this project, LDA created clear linear decision boundaries which contributed to strong and straightforward classification of high and low-risk companies. 

In regards to model limitations, the SVM model’s performance can be sensitive to the choice of kernel and regularization parameters. Thus, optimal parameter selection requires extensive cross-validation which can be computationally and time intensive. LDA has limitations as well. LDA assumes normally distributed classes with identical covariances, but this assumption is not always the case with many data sets. LDA’s reliance on that assumption can hinder its flexibility compared to other models. 

In the context of this project, the choice of a linear kernel for the SVM model may not effectively represent the complex nature of financial data. Future work could explore non-linear kernels or other advanced models, such as ensemble models,  to enhance classification performance. In addition, including more financial indicators in the model could improve the model’s performance as well. Adding more financial indicators and investigating the different impacts of market conditions on classification accuracy would expand the breadth of analysis and further improve the predictive abilities of SVM. 

In conclusion, this project demonstrates the use of SVM for classifying market risk in S&P 500 companies. We determined that SVM effectively classified the market risk of S&P 500 companies with reasonable accuracy and performance. However, the LDA model performed slightly better in this classification task. The results of this project can be replicated by fellow students, investors, and anyone interested in classifying stocks as high or low-risk using statistical learning.  

